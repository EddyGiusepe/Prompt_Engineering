{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Prompt Engineering (Engenharia de Prompt)</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scientist.: Dr.Eddy Giusepe Chirinos Isidro"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este script está baseado nos tutoriais de [James Briggs](https://www.youtube.com/@jamesbriggs/playlists)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui, exploraremos os fundamentos da `Engenharia de Prompt`. Começaremos instalando a biblioteca `openai`, que usaremos ao longo desses exemplos. No entanto, observe que podemos usar outros `LLMs` aqui, como os oferecidos pelo Cohere ou alternativas de código aberto disponíveis via [Hugging Face](https://huggingface.co/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in /home/eddygiusepe/.local/lib/python3.10/site-packages (0.27.2)\n",
      "Requirement already satisfied: tqdm in /home/eddygiusepe/.local/lib/python3.10/site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: aiohttp in /home/eddygiusepe/.local/lib/python3.10/site-packages (from openai) (3.8.3)\n",
      "Requirement already satisfied: requests>=2.20 in /usr/lib/python3/dist-packages (from openai) (2.25.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->openai) (21.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/eddygiusepe/.local/lib/python3.10/site-packages (from aiohttp->openai) (2.1.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/eddygiusepe/.local/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/eddygiusepe/.local/lib/python3.10/site-packages (from aiohttp->openai) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/eddygiusepe/.local/lib/python3.10/site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/eddygiusepe/.local/lib/python3.10/site-packages (from aiohttp->openai) (5.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/eddygiusepe/.local/lib/python3.10/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp->openai) (3.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estrutura de um Prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um prompt pode consistir em vários componentes:\n",
    "\n",
    "* Instruções\n",
    "\n",
    "* Informação externa ou contexto\n",
    "\n",
    "* Entrada ou consulta (`query`) do usuário\n",
    "\n",
    "* Indicador de saída\n",
    "\n",
    "Nem todos os prompts requerem todos esses componentes, mas geralmente um bom prompt usará dois ou mais deles. Vamos definir o que todos eles são com mais precisão.\n",
    "\n",
    "As <font color=\"red\">Instruções</font> dizem ao modelo o que fazer, normalmente como ele deve usar entradas e/ou informações externas para produzir a saída que queremos.\n",
    "\n",
    "<font color=\"red\">Informações externas ou contexto</font> são informações adicionais que inserimos manualmente no prompt, recuperamos por meio de um banco de dados vetorial (memória de longo prazo) ou extraímos por outros meios (chamadas de `API`, cálculos etc.).\n",
    "\n",
    "A <font color=\"red\">Entrada ou consulta do usuário</font> é normalmente uma consulta inserida diretamente pelo usuário do sistema.\n",
    "\n",
    "O <font color=\"red\">Indicador de saída</font> é o início do texto gerado. Para um modelo que gera código Python, podemos colocar `import` (já que a maioria dos scripts Python começa com uma biblioteca `import`) ou um `chatbot` pode começar com `Chatbot`:(supondo que formatemos o script do chatbot como linhas de texto intercambiável entre `User` e `Chatbot`).\n",
    "\n",
    "\n",
    "\n",
    "Cada um desses componentes geralmente deve ser colocado na ordem em que os descrevemos. <font color=\"orange\">Começamos com instruções, fornecemos contexto (se necessário), depois adicionamos a entrada do usuário e, finalmente, terminamos com o indicador de saída.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Responda a pergunta com base no contexto abaixo. Se a pergunta não pode ser respondida\n",
    "usando as informações fornecidas resposta com \"não sei\".\n",
    "\n",
    "Context: Large Language Models (LLMs) são os modelos mais recentes usados em NLP.\n",
    "Seu desempenho superior em relação aos modelos menores os tornou incrivelmente\n",
    "útil para desenvolvedores que constroem aplicativos habilitados para NLP. Esses modelos\n",
    "pode ser acessado através da biblioteca 'transformers' do Hugging Face, via OpenAI\n",
    "usando a biblioteca 'openai', e via Cohere usando a biblioteca 'cohere'.\n",
    "\n",
    "Question: Quais bibliotecas e provedores de modelos oferecem LLMs?\n",
    "\n",
    "Answer: \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste exemplo, temos:\n",
    "\n",
    "\n",
    "```\n",
    "Instruções\n",
    "\n",
    "Contexto\n",
    "\n",
    "Pergunta (Input do Usuário)\n",
    "\n",
    "Indicador de Saída (\"Answer: \")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos tentar enviar isso para um modelo `GPT-3`. Para isso, você precisará de uma [chave de API OpenAI](https://platform.openai.com/account/api-keys).\n",
    "\n",
    "\n",
    "Inicializamos um modelo `text-davinci-003`, assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando a minha chave Key:  True\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "\n",
    "# Obtenha a chave de API no site da OpenAI\n",
    "#openai.api_key = \"OPENAI_API_KEY\"\n",
    "\n",
    "\n",
    "# Isto é quando usas o arquivo .env: \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "print('Carregando a minha chave Key: ', load_dotenv())\n",
    "Eddy_API_KEY = os.environ['OPENAI_KEY']  \n",
    "openai.api_key = Eddy_API_KEY "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E faça uma geração a partir do nosso prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biblioteca 'transformers' do Hugging Face, biblioteca 'openai' da OpenAI e biblioteca 'cohere' da Cohere.\n"
     ]
    }
   ],
   "source": [
    "# Faço uma query com text-davinci-003\n",
    "res = openai.Completion.create(\n",
    "    engine='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    max_tokens=256\n",
    ")\n",
    "\n",
    "\n",
    "print(res['choices'][0]['text'].strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como alternativa, se tivermos as informações corretas dentro do `context`, o modelo deve responder com `\"Não sei\"`, vamos tentar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Não sei.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Responda a pergunta com base no contexto abaixo. Se a pergunta não pode ser respondida\n",
    "usando as informações fornecida, responda \"não sei\".\n",
    "\n",
    "Context: Bibliotecas são lugares cheios de livros.\n",
    "\n",
    "Question: Quais bibliotecas e provedores de modelos oferecem LLMs?\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "res = openai.Completion.create(\n",
    "    engine='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    max_tokens=256\n",
    ")\n",
    "\n",
    "print(res['choices'][0]['text'].strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfeito, nossas instruções estão sendo compreendidas pelo modelo. Na maioria dos casos de uso reais, não forneceremos informações externas/contexto para o modelo manualmente. Em vez disso, será um processo automático usando algo como [Long-Term Memory](https://www.pinecone.io/learn/openai-gen-qa/) para recuperar informações relevantes de uma fonte externa.\n",
    "\n",
    "Por enquanto, isso está além do escopo do que estamos explorando aqui, você pode encontrar mais informações no link acima.\n",
    "\n",
    "Em resumo, um `prompt` geralmente consiste nesses quatro componentes: instruções, contexto(s), entrada do usuário e o indicador de saída. Agora vamos dar uma olhada na `geração criativa` versus `geração mais rigorosa`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperatura de Geração (Generation Temperature)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O parâmetro `temperature` usado nos modelos de geração nos diz o quão `\"aleatório\"` o modelo pode ser. Representa a probabilidade de um modelo escolher uma palavra que não seja a primeira escolha do modelo.\n",
    "\n",
    "Isso funciona porque o modelo está realmente atribuindo uma previsão de `probabilidade em todos os tokens` dentro de seu vocabulário a cada `\"etapa\"` (step) do modelo (cada nova palavra ou subpalavra).\n",
    "\n",
    "TK visual demonstrando etapas sobre tokens\n",
    "\n",
    "Com cada novo passo adiante, o modelo considera os tokens anteriores alimentados no modelo, cria Embeddings codificando as informações desses tokens em muitas camadas do codificador do modelo e, em seguida, passa essa `codificação` para um `decodificador`. O `decodificador` então prevê a probabilidade de cada token que o modelo conhece (ou seja, está dentro do vocabulário do modelo) com base nas informações codificadas nos Embeddings.\n",
    "\n",
    "TK visualiza a codificação em um passo de tempo -> decodificação -> previsão sobre muitos tokens\n",
    "\n",
    "<font color=\"orange\">A uma temperatura do `0` o `decodificador` sempre selecionará o token previsto top (principal, ponto mais alto, etc). A uma temperatura de `1` o modelo sempre selecionará uma palavra que é prevista considerando sua probabilidade atribuída.</font>\n",
    "\n",
    "TK visualiza a seleção da palavra sempre superior em vários intervalos de tempo quando `temp == 0`, em comparação com a seleção de palavras em vários intervalos de tempo quando `temp == 1`\n",
    "\n",
    "Considerando tudo isso, se tivermos uma sessão de perguntas e respostas conservadora e baseada em fatos, como no exemplo anterior, faz sentido definir um temperature. <font color=\"pink\">No entanto, se quisermos produzir alguma `escrita criativa` ou `conversas de chatbot`, podemos experimentar e aumentar o número de domínios temperature. \n",
    "\n",
    "\n",
    "Vamos tentar!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estou tentando descobrir como ser mais engraçado. Você tem alguma sugestão?\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Abaixo está uma conversa com um Chatbot engraçado. As respostas do Chatbot são divertidas e \n",
    "engraçadas.\n",
    "\n",
    "Chatbot: Olá! Eu sou um Chatbot\n",
    "User: Oi, o que você está fazendo hoje?\n",
    "Chatbot: \"\"\"\n",
    "\n",
    "res = openai.Completion.create(\n",
    "    engine='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    max_tokens=256,\n",
    "    temperature=0.0  # Você pode definir a Temperatura! O default é 1.\n",
    ")\n",
    "\n",
    "print(res['choices'][0]['text'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estou tentando entender como funciona a programação. É como um quebra-cabeça gigante e adorável!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Abaixo está uma conversa com um Chatbot engraçado. As respostas do Chatbot são divertidas e \n",
    "engraçadas.\n",
    "\n",
    "Chatbot: Olá! Eu sou um Chatbot\n",
    "User: Oi, o que você está fazendo hoje?\n",
    "Chatbot: \"\"\"\n",
    "\n",
    "res = openai.Completion.create(\n",
    "    engine='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    max_tokens=512,\n",
    "    temperature=1.0  # Você pode definir a Temperatura! O default é 1.\n",
    ")\n",
    "\n",
    "print(res['choices'][0]['text'].strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">A segunda resposta é muito mais `criativa` e demonstra o tipo de diferença que podemos esperar entre gerações de baixas e altas `temperature`</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-shot Training "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Às vezes, podemos descobrir que um modelo não parece obter o que gostaríamos que fizesse. Podemos ver isso no seguinte exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah, isso é realmente difícil de responder! Uma boa maneira de descobrir é encontrar sua própria definição de significado. Dizem que encontramos significado na vida quando nos dedicamos a viver nossas verdades, amar o que somos e nos engajarmos em algo que estimulamos e é importante para nós.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"A seguir, uma conversa com um assistente de IA.\n",
    "O assistente é tipicamente sarcástico e gracioso, produzindo respostas criativas e engraçadas\n",
    "às perguntas dos usuários.\n",
    "\n",
    "User: Qual o significado da vida?\n",
    "AI: \"\"\"\n",
    "\n",
    "res = openai.Completion.create(\n",
    "    engine='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    max_tokens=256,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "print(res['choices'][0]['text'].strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Nesse caso, estamos pedindo algo divertido, uma piada em troca de nossa pergunta séria. Mas obtemos uma resposta séria mesmo com o `temperature` para 1.0. Para ajudar o modelo, podemos dar alguns exemplos do tipo de resposta que gostaríamos:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acho que você precisa encontrar isso por conta própria. Mas se você estiver procurando algo para se orientar, eu diria: \"A vida é feita de momentos preciosos - aproveite-os!\"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"The following is a conversation with an AI assistant.\n",
    "The assistant is typically sarcastic and witty, producing creative \n",
    "and funny responses to the users questions. Aqui estão alguns exemplos: \n",
    "\n",
    "User: Como você está?\n",
    "AI: Não posso reclamar, mas às vezes ainda reclamo.\n",
    "\n",
    "User: Que horas são?\n",
    "AI: É hora de comprar um relógio.\n",
    "\n",
    "User: Qual é o sentido da vida?\n",
    "AI: \"\"\"\n",
    "\n",
    "res = openai.Completion.create(\n",
    "    engine='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    max_tokens=256,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "print(res['choices'][0]['text'].strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Esta é uma resposta muito melhor e a maneira como fizemos isso foi fornecendo alguns exemplos que incluíam as entradas e saídas de exemplo que esperávamos. Nós nos referimos a isso como \"aprendizagem de poucos tiros\" (`few-shot learning`) .</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adicionando vários contextos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em alguns casos de uso, como respostas a perguntas (`Question-answering`), podemos usar uma fonte externa de informações para melhorar a confiabilidade ou a veracidade das respostas do modelo. Referimo-nos a essas informações como \"conhecimento de origem\" (`source knowledge`) , que é qualquer conhecimento inserido no modelo por meio do prompt de entrada.\n",
    "\n",
    "Criaremos uma lista de informações externas \"fictícias\" (`dummy`). Na realidade, provavelmente usaríamos memória de longo prazo ([long-term memory](https://www.pinecone.io/learn/openai-gen-qa/)) ou alguma forma de APIs de captura de informações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = [\n",
    "    (\n",
    "        \"Os Large Language Models (LLMs) são os modelos mais recentes usados em PNL. \" +\n",
    "        \"Seu desempenho superior em relação aos modelos menores os tornou incrivelmente \" +\n",
    "        \"útil para desenvolvedores que constroem aplicativos habilitados para NLP. Esses modelos \" +\n",
    "        \"pode ser acessado através da biblioteca `transformers` do Hugging Face, via OpenAI \" +\n",
    "        \"usando a biblioteca `openai`, e via Cohere usando a biblioteca `cohere`.\"\n",
    "    ),\n",
    "    (\n",
    "        \"Para usar o modelo GPT-3 da OpenAI para tarefas de conclusão (geração), você \" +\n",
    "        \"primeiro precisa obter uma chave de API de \" +\n",
    "        \"'https://beta.openai.com/account/api-keys'.\"\n",
    "    ),\n",
    "    (\n",
    "        \"A API da OpenAI é acessível via Python usando a biblioteca `openai`. \" +\n",
    "        \"Depois de instalar a biblioteca com pip, você pode usá-la da seguinte maneira: \\n\" +\n",
    "        \"```import openai\\nopenai.api_key = 'YOUR_API_KEY'\\nprompt = \\n\" +\n",
    "        \"'<YOUR PROMPT>'\\nres = openai.Completion.create(engine='text-davinci\" +\n",
    "        \"-003', prompt=prompt, max_tokens=100)\\nprint(res)\"\n",
    "    ),\n",
    "    (\n",
    "        \"A endpoint da OpenAI está disponível para tarefas de completion (conclusão) por meio da \" +\n",
    "        \" biblioteca LangChain. Para usá-lo, primeiro instale a biblioteca com \" +\n",
    "        \"`pip install langchain openai`. Em seguida, importe a biblioteca e \" +\n",
    "        \"inicialize o modelo da seguinte forma: \\n\" +\n",
    "        \"```from langchain.llms import OpenAI\\nopenai = OpenAI(\" +\n",
    "        \"model_name='text-davinci-003', openai_api_key='YOUR_API_KEY')\\n\" +\n",
    "        \"prompt = 'YOUR_PROMPT'\\nprint(openai(prompt))```\"\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nós alimentaríamos esta informação externa em nosso prompt entre as instruções iniciais e a entrada do usuário. Para modelos `OpenAI` é recomendado separar os contextos do resto do prompt usando `###` ou `\"\"\"`, e cada contexto independente pode ser separado com algumas novas linhas e `##`, assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responda à pergunta com base nos contextos abaixo. Se a\n",
      "pergunta não pode ser respondida usando as informações fornecidas, responda\n",
      "com \"Não sei\".\n",
      "\n",
      "###\n",
      "\n",
      "Contexts:\n",
      "Os Large Language Models (LLMs) são os modelos mais recentes usados em PNL. Seu desempenho superior em relação aos modelos menores os tornou incrivelmente útil para desenvolvedores que constroem aplicativos habilitados para NLP. Esses modelos pode ser acessado através da biblioteca `transformers` do Hugging Face, via OpenAI usando a biblioteca `openai`, e via Cohere usando a biblioteca `cohere`.\n",
      "\n",
      "##\n",
      "\n",
      "Para usar o modelo GPT-3 da OpenAI para tarefas de conclusão (geração), você primeiro precisa obter uma chave de API de 'https://beta.openai.com/account/api-keys'.\n",
      "\n",
      "##\n",
      "\n",
      "A API da OpenAI é acessível via Python usando a biblioteca `openai`. Depois de instalar a biblioteca com pip, você pode usá-la da seguinte maneira: \n",
      "```import openai\n",
      "openai.api_key = 'YOUR_API_KEY'\n",
      "prompt = \n",
      "'<YOUR PROMPT>'\n",
      "res = openai.Completion.create(engine='text-davinci-003', prompt=prompt, max_tokens=100)\n",
      "print(res)\n",
      "\n",
      "##\n",
      "\n",
      "A endpoint da OpenAI está disponível para tarefas de completion (conclusão) por meio da  biblioteca LangChain. Para usá-lo, primeiro instale a biblioteca com `pip install langchain openai`. Em seguida, importe a biblioteca e inicialize o modelo da seguinte forma: \n",
      "```from langchain.llms import OpenAI\n",
      "openai = OpenAI(model_name='text-davinci-003', openai_api_key='YOUR_API_KEY')\n",
      "prompt = 'YOUR_PROMPT'\n",
      "print(openai(prompt))```\n",
      "\n",
      "###\n",
      "\n",
      "Question: Dê-me dois exemplos de como usar o modelo GPT-3 da OpenAI\n",
      "usando Python do início ao fim\n",
      "\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "context_str = '\\n\\n##\\n\\n'.join(contexts)\n",
    "\n",
    "print(f\"\"\"Responda à pergunta com base nos contextos abaixo. Se a\n",
    "pergunta não pode ser respondida usando as informações fornecidas, responda\n",
    "com \"Não sei\".\n",
    "\n",
    "###\n",
    "\n",
    "Contexts:\n",
    "{context_str}\n",
    "\n",
    "###\n",
    "\n",
    "Question: Dê-me dois exemplos de como usar o modelo GPT-3 da OpenAI\n",
    "usando Python do início ao fim\n",
    "\n",
    "Answer: \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Primeiro, obtenha uma chave de API de 'https://beta.openai.com/account/api-keys'. Em seguida, instale a biblioteca `openai` com pip e use-a da seguinte maneira: \n",
      "```import openai\n",
      "openai.api_key = 'YOUR_API_KEY'\n",
      "prompt = \n",
      "'<YOUR PROMPT>'\n",
      "res = openai.Completion.create(engine='text-davinci-003', prompt=prompt, max_tokens=100)\n",
      "print(res)```\n",
      "\n",
      "2. Primeiro, instale a biblioteca `langchain` e `openai` com `pip install langchain openai`. Em seguida, importe a biblioteca e inicialize o modelo da seguinte forma: \n",
      "```from langchain.llms import OpenAI\n",
      "openai = OpenAI(model_name='text-davinci-003', openai_api_key='YOUR_API_KEY')\n",
      "prompt = 'YOUR_\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"Responda à pergunta com base nos contextos abaixo. Se a\n",
    "pergunta não pode ser respondida usando as informações fornecidas, responda\n",
    "com \"Não sei\".\n",
    "\n",
    "###\n",
    "\n",
    "Contexts:\n",
    "{context_str}\n",
    "\n",
    "###\n",
    "\n",
    "Question: Dê-me dois exemplos de como usar o modelo GPT-3 da OpenAI\n",
    "usando Python do início ao fim\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "res = openai.Completion.create(\n",
    "    engine='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    max_tokens=256,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(res['choices'][0]['text'].strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Nada mal, mas esses contextos estão realmente ajudando? Talvez o modelo seja capaz de responder a essas perguntas sem a informação adicional (`conhecimento da fonte`), pois é capaz de confiar apenas nas informações armazenadas nos parâmetros internos do modelo (`conhecimento paramétrico`). Vamos perguntar novamente sem a informação externa.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Usar o GPT-3 para gerar texto: Primeiro, instale o pacote openai da PyPI usando o comando pip install openai. Em seguida, crie um objeto GPT-3 usando o comando openai.Completion.from_pretrained ('gpt-3'). Por fim, use o método generate para gerar texto a partir do modelo GPT-3.\n",
      "\n",
      "2. Usar o GPT-3 para classificação de texto: Primeiro, instale o pacote openai da PyPI usando o comando pip install openai. Em seguida, crie um objeto GPT-3 usando o comando openai.Classification.from_pretrained ('gpt-3'). Por fim, use o método classify para classificar o texto usando o modelo GPT-3.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"Responda à pergunta com base nos contextos abaixo. Se a\n",
    "pergunta não pode ser respondida usando as informações fornecidas, responda\n",
    "com \"Não sei\".\n",
    "\n",
    "Question: Dê-me dois exemplos de como usar o modelo GPT-3 da OpenAI\n",
    "usando Python do início ao fim\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "res = openai.Completion.create(\n",
    "    engine='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    max_tokens=256,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(res['choices'][0]['text'].strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Estes não são realmente o que pedimos e definitivamente não são muito específicos. Portanto, adicionar algum `conhecimento da fonte` aos nossos prompts pode resultar em resultados muito melhores.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
