{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Prompt Engineering (Engenharia de Prompt)</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scientist.: Dr.Eddy Giusepe Chirinos Isidro"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este script está baseado nos tutoriais de [James Briggs](https://www.youtube.com/@jamesbriggs/playlists)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui, exploraremos os fundamentos da `Engenharia de Prompt`. Começaremos instalando a biblioteca `openai`, que usaremos ao longo desses exemplos. No entanto, observe que podemos usar outros `LLMs` aqui, como os oferecidos pelo Cohere ou alternativas de código aberto disponíveis via [Hugging Face](https://huggingface.co/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in /home/eddygiusepe/.local/lib/python3.10/site-packages (0.27.2)\n",
      "Requirement already satisfied: tqdm in /home/eddygiusepe/.local/lib/python3.10/site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: aiohttp in /home/eddygiusepe/.local/lib/python3.10/site-packages (from openai) (3.8.3)\n",
      "Requirement already satisfied: requests>=2.20 in /usr/lib/python3/dist-packages (from openai) (2.25.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->openai) (21.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/eddygiusepe/.local/lib/python3.10/site-packages (from aiohttp->openai) (2.1.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/eddygiusepe/.local/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/eddygiusepe/.local/lib/python3.10/site-packages (from aiohttp->openai) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/eddygiusepe/.local/lib/python3.10/site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/eddygiusepe/.local/lib/python3.10/site-packages (from aiohttp->openai) (5.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/eddygiusepe/.local/lib/python3.10/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp->openai) (3.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estrutura de um Prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um prompt pode consistir em vários componentes:\n",
    "\n",
    "* Instruções\n",
    "\n",
    "* Informação externa ou contexto\n",
    "\n",
    "* Entrada ou consulta (`query`) do usuário\n",
    "\n",
    "* Indicador de saída\n",
    "\n",
    "Nem todos os prompts requerem todos esses componentes, mas geralmente um bom prompt usará dois ou mais deles. Vamos definir o que todos eles são com mais precisão.\n",
    "\n",
    "As <font color=\"red\">Instruções</font> dizem ao modelo o que fazer, normalmente como ele deve usar entradas e/ou informações externas para produzir a saída que queremos.\n",
    "\n",
    "<font color=\"red\">Informações externas ou contexto</font> são informações adicionais que inserimos manualmente no prompt, recuperamos por meio de um banco de dados vetorial (memória de longo prazo) ou extraímos por outros meios (chamadas de `API`, cálculos etc.).\n",
    "\n",
    "A <font color=\"red\">Entrada ou consulta do usuário</font> é normalmente uma consulta inserida diretamente pelo usuário do sistema.\n",
    "\n",
    "O <font color=\"red\">Indicador de saída</font> é o início do texto gerado. Para um modelo que gera código Python, podemos colocar `import` (já que a maioria dos scripts Python começa com uma biblioteca `import`) ou um `chatbot` pode começar com `Chatbot`:(supondo que formatemos o script do chatbot como linhas de texto intercambiável entre `User` e `Chatbot`).\n",
    "\n",
    "\n",
    "\n",
    "Cada um desses componentes geralmente deve ser colocado na ordem em que os descrevemos. <font color=\"orange\">Começamos com instruções, fornecemos contexto (se necessário), depois adicionamos a entrada do usuário e, finalmente, terminamos com o indicador de saída.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Responda a pergunta com base no contexto abaixo. Se a pergunta não pode ser respondida\n",
    "usando as informações fornecidas resposta com \"não sei\".\n",
    "\n",
    "Context: Large Language Models (LLMs) são os modelos mais recentes usados em NLP.\n",
    "Seu desempenho superior em relação aos modelos menores os tornou incrivelmente\n",
    "útil para desenvolvedores que constroem aplicativos habilitados para NLP. Esses modelos\n",
    "pode ser acessado através da biblioteca 'transformers' do Hugging Face, via OpenAI\n",
    "usando a biblioteca 'openai', e via Cohere usando a biblioteca 'cohere'.\n",
    "\n",
    "Question: Quais bibliotecas e provedores de modelos oferecem LLMs?\n",
    "\n",
    "Answer: \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste exemplo, temos:\n",
    "\n",
    "\n",
    "```\n",
    "Instruções\n",
    "\n",
    "Contexto\n",
    "\n",
    "Pergunta (Input do Usuário)\n",
    "\n",
    "Indicador de Saída (\"Answer: \")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos tentar enviar isso para um modelo `GPT-3`. Para isso, você precisará de uma [chave de API OpenAI](https://platform.openai.com/account/api-keys).\n",
    "\n",
    "\n",
    "Inicializamos um modelo `text-davinci-003`, assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando a minha chave Key:  True\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "\n",
    "# Obtenha a chave de API no site da OpenAI\n",
    "#openai.api_key = \"OPENAI_API_KEY\"\n",
    "\n",
    "\n",
    "# Isto é quando usas o arquivo .env: \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "print('Carregando a minha chave Key: ', load_dotenv())\n",
    "Eddy_API_KEY = os.environ['OPENAI_KEY']  \n",
    "openai.api_key = Eddy_API_KEY "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E faça uma geração a partir do nosso prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biblioteca 'transformers' do Hugging Face, biblioteca 'openai' da OpenAI e biblioteca 'cohere' da Cohere.\n"
     ]
    }
   ],
   "source": [
    "# Faço uma query com text-davinci-003\n",
    "res = openai.Completion.create(\n",
    "    engine='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    max_tokens=256\n",
    ")\n",
    "\n",
    "\n",
    "print(res['choices'][0]['text'].strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como alternativa, se tivermos as informações corretas dentro do `context`, o modelo deve responder com `\"Não sei\"`, vamos tentar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Não sei.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Responda a pergunta com base no contexto abaixo. Se a pergunta não pode ser respondida\n",
    "usando as informações fornecida, responda \"não sei\".\n",
    "\n",
    "Context: Bibliotecas são lugares cheios de livros.\n",
    "\n",
    "Question: Quais bibliotecas e provedores de modelos oferecem LLMs?\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "res = openai.Completion.create(\n",
    "    engine='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    max_tokens=256\n",
    ")\n",
    "\n",
    "print(res['choices'][0]['text'].strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfeito, nossas instruções estão sendo compreendidas pelo modelo. Na maioria dos casos de uso reais, não forneceremos informações externas/contexto para o modelo manualmente. Em vez disso, será um processo automático usando algo como [Long-Term Memory](https://www.pinecone.io/learn/openai-gen-qa/) para recuperar informações relevantes de uma fonte externa.\n",
    "\n",
    "Por enquanto, isso está além do escopo do que estamos explorando aqui, você pode encontrar mais informações no link acima.\n",
    "\n",
    "Em resumo, um `prompt` geralmente consiste nesses quatro componentes: instruções, contexto(s), entrada do usuário e o indicador de saída. Agora vamos dar uma olhada na `geração criativa` versus `geração mais rigorosa`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperatura de Geração (Generation Temperature)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O parâmetro `temperature` usado nos modelos de geração nos diz o quão `\"aleatório\"` o modelo pode ser. Representa a probabilidade de um modelo escolher uma palavra que não seja a primeira escolha do modelo.\n",
    "\n",
    "Isso funciona porque o modelo está realmente atribuindo uma previsão de `probabilidade em todos os tokens` dentro de seu vocabulário a cada `\"etapa\"` (step) do modelo (cada nova palavra ou subpalavra).\n",
    "\n",
    "TK visual demonstrando etapas sobre tokens\n",
    "\n",
    "Com cada novo passo adiante, o modelo considera os tokens anteriores alimentados no modelo, cria Embeddings codificando as informações desses tokens em muitas camadas do codificador do modelo e, em seguida, passa essa `codificação` para um `decodificador`. O `decodificador` então prevê a probabilidade de cada token que o modelo conhece (ou seja, está dentro do vocabulário do modelo) com base nas informações codificadas nos Embeddings.\n",
    "\n",
    "TK visualiza a codificação em um passo de tempo -> decodificação -> previsão sobre muitos tokens\n",
    "\n",
    "<font color=\"orange\">A uma temperatura do `0` o `decodificador` sempre selecionará o token previsto top (principal, ponto mais alto, etc). A uma temperatura de `1` o modelo sempre selecionará uma palavra que é prevista considerando sua probabilidade atribuída.</font>\n",
    "\n",
    "TK visualiza a seleção da palavra sempre superior em vários intervalos de tempo quando `temp == 0`, em comparação com a seleção de palavras em vários intervalos de tempo quando `temp == 1`\n",
    "\n",
    "Considerando tudo isso, se tivermos uma sessão de perguntas e respostas conservadora e baseada em fatos, como no exemplo anterior, faz sentido definir um temperature. <font color=\"pink\">No entanto, se quisermos produzir alguma `escrita criativa` ou `conversas de chatbot`, podemos experimentar e aumentar o número de domínios temperature. \n",
    "\n",
    "\n",
    "Vamos tentar!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estou tentando descobrir como ser mais engraçado. Você tem alguma sugestão?\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Abaixo está uma conversa com um Chatbot engraçado. As respostas do Chatbot são divertidas e \n",
    "engraçadas.\n",
    "\n",
    "Chatbot: Olá! Eu sou um Chatbot\n",
    "User: Oi, o que você está fazendo hoje?\n",
    "Chatbot: \"\"\"\n",
    "\n",
    "res = openai.Completion.create(\n",
    "    engine='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    max_tokens=256,\n",
    "    temperature=0.0  # Você pode definir a Temperatura! O default é 1.\n",
    ")\n",
    "\n",
    "print(res['choices'][0]['text'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estou tentando entender como funciona a programação. É como um quebra-cabeça gigante e adorável!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Abaixo está uma conversa com um Chatbot engraçado. As respostas do Chatbot são divertidas e \n",
    "engraçadas.\n",
    "\n",
    "Chatbot: Olá! Eu sou um Chatbot\n",
    "User: Oi, o que você está fazendo hoje?\n",
    "Chatbot: \"\"\"\n",
    "\n",
    "res = openai.Completion.create(\n",
    "    engine='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    max_tokens=512,\n",
    "    temperature=1.0  # Você pode definir a Temperatura! O default é 1.\n",
    ")\n",
    "\n",
    "print(res['choices'][0]['text'].strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">A segunda resposta é muito mais `criativa` e demonstra o tipo de diferença que podemos esperar entre gerações de baixas e altas `temperature`</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-shot Training "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Às vezes, podemos descobrir que um modelo não parece obter o que gostaríamos que fizesse. Podemos ver isso no seguinte exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah, isso é realmente difícil de responder! Uma boa maneira de descobrir é encontrar sua própria definição de significado. Dizem que encontramos significado na vida quando nos dedicamos a viver nossas verdades, amar o que somos e nos engajarmos em algo que estimulamos e é importante para nós.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"A seguir, uma conversa com um assistente de IA.\n",
    "O assistente é tipicamente sarcástico e gracioso, produzindo respostas criativas e engraçadas\n",
    "às perguntas dos usuários.\n",
    "\n",
    "User: Qual o significado da vida?\n",
    "AI: \"\"\"\n",
    "\n",
    "res = openai.Completion.create(\n",
    "    engine='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    max_tokens=256,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "print(res['choices'][0]['text'].strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Nesse caso, estamos pedindo algo divertido, uma piada em troca de nossa pergunta séria. Mas obtemos uma resposta séria mesmo com o `temperature` para 1.0. Para ajudar o modelo, podemos dar alguns exemplos do tipo de resposta que gostaríamos:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acho que você precisa encontrar isso por conta própria. Mas se você estiver procurando algo para se orientar, eu diria: \"A vida é feita de momentos preciosos - aproveite-os!\"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"The following is a conversation with an AI assistant.\n",
    "The assistant is typically sarcastic and witty, producing creative \n",
    "and funny responses to the users questions. Aqui estão alguns exemplos: \n",
    "\n",
    "User: Como você está?\n",
    "AI: Não posso reclamar, mas às vezes ainda reclamo.\n",
    "\n",
    "User: Que horas são?\n",
    "AI: É hora de comprar um relógio.\n",
    "\n",
    "User: Qual é o sentido da vida?\n",
    "AI: \"\"\"\n",
    "\n",
    "res = openai.Completion.create(\n",
    "    engine='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    max_tokens=256,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "print(res['choices'][0]['text'].strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Esta é uma resposta muito melhor e a maneira como fizemos isso foi fornecendo alguns exemplos que incluíam as entradas e saídas de exemplo que esperávamos. Nós nos referimos a isso como \"aprendizagem de poucos tiros\" (`few-shot learning`) .</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adicionando vários contextos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em alguns casos de uso, como respostas a perguntas (`Question-answering`), podemos usar uma fonte externa de informações para melhorar a confiabilidade ou a veracidade das respostas do modelo. Referimo-nos a essas informações como \"conhecimento de origem\" (`source knowledge`) , que é qualquer conhecimento inserido no modelo por meio do prompt de entrada.\n",
    "\n",
    "Criaremos uma lista de informações externas \"fictícias\" (`dummy`). Na realidade, provavelmente usaríamos memória de longo prazo ([long-term memory](https://www.pinecone.io/learn/openai-gen-qa/)) ou alguma forma de APIs de captura de informações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = [\n",
    "    (\n",
    "        \"Os Large Language Models (LLMs) são os modelos mais recentes usados em PNL. \" +\n",
    "        \"Seu desempenho superior em relação aos modelos menores os tornou incrivelmente \" +\n",
    "        \"útil para desenvolvedores que constroem aplicativos habilitados para NLP. Esses modelos \" +\n",
    "        \"pode ser acessado através da biblioteca `transformers` do Hugging Face, via OpenAI \" +\n",
    "        \"usando a biblioteca `openai`, e via Cohere usando a biblioteca `cohere`.\"\n",
    "    ),\n",
    "    (\n",
    "        \"Para usar o modelo GPT-3 da OpenAI para tarefas de conclusão (geração), você \" +\n",
    "        \"primeiro precisa obter uma chave de API de \" +\n",
    "        \"'https://beta.openai.com/account/api-keys'.\"\n",
    "    ),\n",
    "    (\n",
    "        \"A API da OpenAI é acessível via Python usando a biblioteca `openai`. \" +\n",
    "        \"Depois de instalar a biblioteca com pip, você pode usá-la da seguinte maneira: \\n\" +\n",
    "        \"```import openai\\nopenai.api_key = 'YOUR_API_KEY'\\nprompt = \\n\" +\n",
    "        \"'<YOUR PROMPT>'\\nres = openai.Completion.create(engine='text-davinci\" +\n",
    "        \"-003', prompt=prompt, max_tokens=100)\\nprint(res)\"\n",
    "    ),\n",
    "    (\n",
    "        \"A endpoint da OpenAI está disponível para tarefas de completion (conclusão) por meio da \" +\n",
    "        \" biblioteca LangChain. Para usá-lo, primeiro instale a biblioteca com \" +\n",
    "        \"`pip install langchain openai`. Em seguida, importe a biblioteca e \" +\n",
    "        \"inicialize o modelo da seguinte forma: \\n\" +\n",
    "        \"```from langchain.llms import OpenAI\\nopenai = OpenAI(\" +\n",
    "        \"model_name='text-davinci-003', openai_api_key='YOUR_API_KEY')\\n\" +\n",
    "        \"prompt = 'YOUR_PROMPT'\\nprint(openai(prompt))```\"\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nós alimentaríamos esta informação externa em nosso prompt entre as instruções iniciais e a entrada do usuário. Para modelos `OpenAI` é recomendado separar os contextos do resto do prompt usando `###` ou `\"\"\"`, e cada contexto independente pode ser separado com algumas novas linhas e `##`, assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responda à pergunta com base nos contextos abaixo. Se a\n",
      "pergunta não pode ser respondida usando as informações fornecidas, responda\n",
      "com \"Não sei\".\n",
      "\n",
      "###\n",
      "\n",
      "Contexts:\n",
      "Os Large Language Models (LLMs) são os modelos mais recentes usados em PNL. Seu desempenho superior em relação aos modelos menores os tornou incrivelmente útil para desenvolvedores que constroem aplicativos habilitados para NLP. Esses modelos pode ser acessado através da biblioteca `transformers` do Hugging Face, via OpenAI usando a biblioteca `openai`, e via Cohere usando a biblioteca `cohere`.\n",
      "\n",
      "##\n",
      "\n",
      "Para usar o modelo GPT-3 da OpenAI para tarefas de conclusão (geração), você primeiro precisa obter uma chave de API de 'https://beta.openai.com/account/api-keys'.\n",
      "\n",
      "##\n",
      "\n",
      "A API da OpenAI é acessível via Python usando a biblioteca `openai`. Depois de instalar a biblioteca com pip, você pode usá-la da seguinte maneira: \n",
      "```import openai\n",
      "openai.api_key = 'YOUR_API_KEY'\n",
      "prompt = \n",
      "'<YOUR PROMPT>'\n",
      "res = openai.Completion.create(engine='text-davinci-003', prompt=prompt, max_tokens=100)\n",
      "print(res)\n",
      "\n",
      "##\n",
      "\n",
      "A endpoint da OpenAI está disponível para tarefas de completion (conclusão) por meio da  biblioteca LangChain. Para usá-lo, primeiro instale a biblioteca com `pip install langchain openai`. Em seguida, importe a biblioteca e inicialize o modelo da seguinte forma: \n",
      "```from langchain.llms import OpenAI\n",
      "openai = OpenAI(model_name='text-davinci-003', openai_api_key='YOUR_API_KEY')\n",
      "prompt = 'YOUR_PROMPT'\n",
      "print(openai(prompt))```\n",
      "\n",
      "###\n",
      "\n",
      "Question: Dê-me dois exemplos de como usar o modelo GPT-3 da OpenAI\n",
      "usando Python do início ao fim\n",
      "\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "context_str = '\\n\\n##\\n\\n'.join(contexts)\n",
    "\n",
    "print(f\"\"\"Responda à pergunta com base nos contextos abaixo. Se a\n",
    "pergunta não pode ser respondida usando as informações fornecidas, responda\n",
    "com \"Não sei\".\n",
    "\n",
    "###\n",
    "\n",
    "Contexts:\n",
    "{context_str}\n",
    "\n",
    "###\n",
    "\n",
    "Question: Dê-me dois exemplos de como usar o modelo GPT-3 da OpenAI\n",
    "usando Python do início ao fim\n",
    "\n",
    "Answer: \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Primeiro, obtenha uma chave de API de 'https://beta.openai.com/account/api-keys'. Em seguida, instale a biblioteca `openai` com pip e use-a da seguinte maneira: \n",
      "```import openai\n",
      "openai.api_key = 'YOUR_API_KEY'\n",
      "prompt = \n",
      "'<YOUR PROMPT>'\n",
      "res = openai.Completion.create(engine='text-davinci-003', prompt=prompt, max_tokens=100)\n",
      "print(res)```\n",
      "\n",
      "2. Primeiro, instale a biblioteca `langchain` e `openai` com `pip install langchain openai`. Em seguida, importe a biblioteca e inicialize o modelo da seguinte forma: \n",
      "```from langchain.llms import OpenAI\n",
      "openai = OpenAI(model_name='text-davinci-003', openai_api_key='YOUR_API_KEY')\n",
      "prompt = 'YOUR_\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"Responda à pergunta com base nos contextos abaixo. Se a\n",
    "pergunta não pode ser respondida usando as informações fornecidas, responda\n",
    "com \"Não sei\".\n",
    "\n",
    "###\n",
    "\n",
    "Contexts:\n",
    "{context_str}\n",
    "\n",
    "###\n",
    "\n",
    "Question: Dê-me dois exemplos de como usar o modelo GPT-3 da OpenAI\n",
    "usando Python do início ao fim\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "res = openai.Completion.create(\n",
    "    engine='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    max_tokens=256,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(res['choices'][0]['text'].strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Nada mal, mas esses contextos estão realmente ajudando? Talvez o modelo seja capaz de responder a essas perguntas sem a informação adicional (`conhecimento da fonte`), pois é capaz de confiar apenas nas informações armazenadas nos parâmetros internos do modelo (`conhecimento paramétrico`). Vamos perguntar novamente sem a informação externa.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Usar o GPT-3 para gerar texto: Primeiro, instale o pacote openai da PyPI usando o comando pip install openai. Em seguida, crie um objeto GPT-3 usando o comando openai.Completion.from_pretrained ('gpt-3'). Por fim, use o método generate para gerar texto a partir do modelo GPT-3.\n",
      "\n",
      "2. Usar o GPT-3 para classificação de texto: Primeiro, instale o pacote openai da PyPI usando o comando pip install openai. Em seguida, crie um objeto GPT-3 usando o comando openai.Classification.from_pretrained ('gpt-3'). Por fim, use o método classify para classificar o texto usando o modelo GPT-3.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"Responda à pergunta com base nos contextos abaixo. Se a\n",
    "pergunta não pode ser respondida usando as informações fornecidas, responda\n",
    "com \"Não sei\".\n",
    "\n",
    "Question: Dê-me dois exemplos de como usar o modelo GPT-3 da OpenAI\n",
    "usando Python do início ao fim\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "res = openai.Completion.create(\n",
    "    engine='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    max_tokens=256,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(res['choices'][0]['text'].strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Estes não são realmente o que pedimos e definitivamente não são muito específicos. Portanto, adicionar algum `conhecimento da fonte` aos nossos prompts pode resultar em resultados muito melhores.</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tamanhos máximos de prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerando que podemos querer fornecer informações externas aos nossos prompts, eles podem naturalmente se tornar bastante grandes. Com isso precisamos perguntar `o quão grande nossos prompts podem ser`, pois existe um tamanho máximo.\n",
    "\n",
    "A janela de contexto máxima de um `LLM` refere-se a `Tokens` no prompt e no texto de completion (conclusão). Para `text-davinci-003` é `4097` tokens.\n",
    "\n",
    "Podemos definir o comprimento máximo de completion do nosso modelo usando `openai.max_tokens = 123`. No entanto, medir o número total de tokens de entrada é mais complexo.\n",
    "\n",
    "Como os tokens não são mapeados diretamente para as palavras, só podemos medir `o número de tokens do texto` ao realmente tokenizar o texto. Os modelos `GPT` usam o `tokenizer TikToken da OpenAI` ([OpenAI's TikToken tokenizer](https://github.com/openai/tiktoken)). Podemos instalar a biblioteca via Pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "591"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "prompt = f\"\"\"Responda à pergunta com base nos contextos abaixo. Se a\n",
    "pergunta não pode ser respondida usando as informações fornecidas, responda\n",
    "com \"Não sei\".\n",
    "\n",
    "###\n",
    "\n",
    "Contexts:\n",
    "{'##'.join(contexts)}\n",
    "\n",
    "###\n",
    "\n",
    "Question: Dê-me dois exemplos de como usar o modelo GPT-3 da OpenAI\n",
    "usando Python do início ao fim\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "encoder_name = 'p50k_base'\n",
    "tokenizer = tiktoken.get_encoding(encoder_name)\n",
    "\n",
    "len(tokenizer.encode(prompt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao alimentar este prompt em `text-davinci-003` ele usará `591`  de nossa janela de contexto máxima de `4097`, deixando-nos com `4097 - 591 == 3506` tokens para nossa completion (conclusão).\n",
    "\n",
    "---\n",
    "\n",
    "* Nem todos os modelos OpenAI usam o encoder `p50k_base`, uma tabela de diferentes encoders para diferentes modelos pode ser encontrada `abaixo`, no momento que escrevo, eles são:*\n",
    "\n",
    "| Encoding name | OpenAI models |\n",
    "| --- | --- |\n",
    "| `gpt2` (or `r50k_base`) | Most GPT-3 models (and GPT-2) |\n",
    "| `p50k_base` | Code models, `text-davinci-002`, `text-davinci-003` |\n",
    "| `cl100k_base` | `text-embedding-ada-002` |\n",
    "\n",
    "---\n",
    "\n",
    "Por padrão,o número máximo de Tokens usados para `completion` é `256`. Podemos aumentar isso até o máximo calculado acima de `3506`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Importe a biblioteca `openai` com `pip` e defina a chave de API: \n",
      "```import openai\n",
      "openai.api_key = 'YOUR_API_KEY'```\n",
      "\n",
      "2. Importe a biblioteca `langchain` e inicialize o modelo: \n",
      "```from langchain.llms import OpenAI\n",
      "openai = OpenAI(model_name='text-davinci-003', openai_api_key='YOUR_API_KEY')```\n"
     ]
    }
   ],
   "source": [
    "res = openai.Completion.create(\n",
    "    engine='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    temperature=0.0,\n",
    "    max_tokens=3506\n",
    ")\n",
    "\n",
    "\n",
    "print(res['choices'][0]['text'].strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">O modelo não precisa do tamanho total de completion (conclusão) e não tenta preencher todo o espaço, mas como aumentamos o valor de `openai.max_tokens`, a inferência leva muito mais tempo.\n",
    "\n",
    "\n",
    "`Se excedermos a janela de contexto máxima permitida, veremos um erro.`</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 4097 tokens, however you requested 4098 tokens (591 in your prompt; 3507 for the completion). Please reduce your prompt; or completion length.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m      2\u001b[0m     engine\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtext-davinci-003\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m     prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[1;32m      4\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m     max_tokens\u001b[39m=\u001b[39;49m\u001b[39m3507\u001b[39;49m \u001b[39m# Aqui aumentamos os Tokens mais do devido (veremos um erro!!!)\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(res[\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstrip())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/api_requestor.py:226\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    217\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    218\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    225\u001b[0m     )\n\u001b[0;32m--> 226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/api_requestor.py:619\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    612\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    613\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    614\u001b[0m         )\n\u001b[1;32m    615\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    616\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    618\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 619\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    620\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    621\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    622\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    623\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    624\u001b[0m         ),\n\u001b[1;32m    625\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    626\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/api_requestor.py:682\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    680\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    681\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 682\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    683\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4097 tokens, however you requested 4098 tokens (591 in your prompt; 3507 for the completion). Please reduce your prompt; or completion length."
     ]
    }
   ],
   "source": [
    "res = openai.Completion.create(\n",
    "    engine='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    temperature=0.0,\n",
    "    max_tokens=3507 # Aqui aumentamos os Tokens mais do devido (veremos um erro!!!)\n",
    ")\n",
    "\n",
    "print(res['choices'][0]['text'].strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">Portanto, pode ser uma boa ideia integrar esse tipo de verificação em nosso código se esperamos exceder a janela de contexto máxima em qualquer ponto.</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
