{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Prompt Engineering (Engenharia de Prompt)</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scientist.: Dr.Eddy Giusepe Chirinos Isidro"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este script está baseado nos tutoriais de [James Briggs](https://www.youtube.com/@jamesbriggs/playlists)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui, exploraremos os fundamentos da `Engenharia de Prompt`. Começaremos instalando a biblioteca `openai`, que usaremos ao longo desses exemplos. No entanto, observe que podemos usar outros `LLMs` aqui, como os oferecidos pelo Cohere ou alternativas de código aberto disponíveis via [Hugging Face](https://huggingface.co/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estrutura de um Prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um prompt pode consistir em vários componentes:\n",
    "\n",
    "* Instruções\n",
    "\n",
    "* Informação externa ou contexto\n",
    "\n",
    "* Entrada ou consulta (`query`) do usuário\n",
    "\n",
    "* Indicador de saída\n",
    "\n",
    "Nem todos os prompts requerem todos esses componentes, mas geralmente um bom prompt usará dois ou mais deles. Vamos definir o que todos eles são com mais precisão.\n",
    "\n",
    "As <font color=\"red\">Instruções</font> dizem ao modelo o que fazer, normalmente como ele deve usar entradas e/ou informações externas para produzir a saída que queremos.\n",
    "\n",
    "<font color=\"red\">Informações externas ou contexto</font> são informações adicionais que inserimos manualmente no prompt, recuperamos por meio de um banco de dados vetorial (memória de longo prazo) ou extraímos por outros meios (chamadas de `API`, cálculos etc.).\n",
    "\n",
    "A <font color=\"red\">Entrada ou consulta do usuário</font> é normalmente uma consulta inserida diretamente pelo usuário do sistema.\n",
    "\n",
    "O <font color=\"red\">Indicador de saída</font> é o início do texto gerado. Para um modelo que gera código Python, podemos colocar `import` (já que a maioria dos scripts Python começa com uma biblioteca `import`) ou um `chatbot` pode começar com `Chatbot`:(supondo que formatemos o script do chatbot como linhas de texto intercambiável entre `User` e `Chatbot`).\n",
    "\n",
    "\n",
    "\n",
    "Cada um desses componentes geralmente deve ser colocado na ordem em que os descrevemos. <font color=\"orange\">Começamos com instruções, fornecemos contexto (se necessário), depois adicionamos a entrada do usuário e, finalmente, terminamos com o indicador de saída.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Responda a pergunta com base no contexto abaixo. Se a pergunta não pode ser respondida\n",
    "usando as informações fornecidas resposta com \"não sei\".\n",
    "\n",
    "Context: Large Language Models (LLMs) são os modelos mais recentes usados em NLP.\n",
    "Seu desempenho superior em relação aos modelos menores os tornou incrivelmente\n",
    "útil para desenvolvedores que constroem aplicativos habilitados para NLP. Esses modelos\n",
    "pode ser acessado através da biblioteca 'transformers' do Hugging Face, via OpenAI\n",
    "usando a biblioteca 'openai', e via Cohere usando a biblioteca 'cohere'.\n",
    "\n",
    "Question: Quais bibliotecas e provedores de modelos oferecem LLMs?\n",
    "\n",
    "Answer: \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste exemplo, temos:\n",
    "\n",
    "\n",
    "```\n",
    "Instruções\n",
    "\n",
    "Contexto\n",
    "\n",
    "Pergunta (Input do Usuário)\n",
    "\n",
    "Indicador de Saída (\"Answer: \")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos tentar enviar isso para um modelo `GPT-3`. Para isso, você precisará de uma [chave de API OpenAI](https://platform.openai.com/account/api-keys).\n",
    "\n",
    "\n",
    "Inicializamos um modelo `text-davinci-003`, assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Obtenha a chave de API no site da OpenAI\n",
    "openai.api_key = \"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\" # \"OPENAI_API_KEY\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E faça uma geração a partir do nosso prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers do Hugging Face, OpenAI e Cohere.\n"
     ]
    }
   ],
   "source": [
    "# Faço uma query com text-davinci-003\n",
    "res = openai.Completion.create(\n",
    "    engine='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    max_tokens=256\n",
    ")\n",
    "\n",
    "\n",
    "print(res['choices'][0]['text'].strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como alternativa, se tivermos as informações corretas dentro do `context`, o modelo deve responder com `\"Não sei\"`, vamos tentar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Não sei.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Responda a pergunta com base no contexto abaixo. Se a pergunta não pode ser respondida\n",
    "usando as informações fornecida, responda \"não sei\".\n",
    "\n",
    "Context: Bibliotecas são lugares cheios de livros.\n",
    "\n",
    "Question: Quais bibliotecas e provedores de modelos oferecem LLMs?\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "res = openai.Completion.create(\n",
    "    engine='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    max_tokens=256\n",
    ")\n",
    "\n",
    "print(res['choices'][0]['text'].strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfeito, nossas instruções estão sendo compreendidas pelo modelo. Na maioria dos casos de uso reais, não forneceremos informações externas/contexto para o modelo manualmente. Em vez disso, será um processo automático usando algo como [Long-Term Memory](https://www.pinecone.io/learn/openai-gen-qa/) para recuperar informações relevantes de uma fonte externa.\n",
    "\n",
    "Por enquanto, isso está além do escopo do que estamos explorando aqui, você pode encontrar mais informações no link acima.\n",
    "\n",
    "Em resumo, um `prompt` geralmente consiste nesses quatro componentes: instruções, contexto(s), entrada do usuário e o indicador de saída. Agora vamos dar uma olhada na `geração criativa` versus `geração mais rigorosa`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperatura de Geração (Generation Temperature)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O parâmetro `temperature` usado nos modelos de geração nos diz o quão `\"aleatório\"` o modelo pode ser. Representa a probabilidade de um modelo escolher uma palavra que não seja a primeira escolha do modelo.\n",
    "\n",
    "Isso funciona porque o modelo está realmente atribuindo uma previsão de `probabilidade em todos os tokens` dentro de seu vocabulário a cada `\"etapa\"` (step) do modelo (cada nova palavra ou subpalavra).\n",
    "\n",
    "TK visual demonstrando etapas sobre tokens\n",
    "\n",
    "Com cada novo passo adiante, o modelo considera os tokens anteriores alimentados no modelo, cria Embeddings codificando as informações desses tokens em muitas camadas do codificador do modelo e, em seguida, passa essa `codificação` para um `decodificador`. O `decodificador` então prevê a probabilidade de cada token que o modelo conhece (ou seja, está dentro do vocabulário do modelo) com base nas informações codificadas nos Embeddings.\n",
    "\n",
    "TK visualiza a codificação em um passo de tempo -> decodificação -> previsão sobre muitos tokens\n",
    "\n",
    "<font color=\"orange\">A uma temperatura do `0` o `decodificador` sempre selecionará o token previsto top (principal, ponto mais alto, etc). A uma temperatura de `1` o modelo sempre selecionará uma palavra que é prevista considerando sua probabilidade atribuída.</font>\n",
    "\n",
    "TK visualiza a seleção da palavra sempre superior em vários intervalos de tempo quando `temp == 0`, em comparação com a seleção de palavras em vários intervalos de tempo quando `temp == 1`\n",
    "\n",
    "Considerando tudo isso, se tivermos uma sessão de perguntas e respostas conservadora e baseada em fatos, como no exemplo anterior, faz sentido definir um temperature. <font color=\"pink\">No entanto, se quisermos produzir alguma `escrita criativa` ou `conversas de chatbot`, podemos experimentar e aumentar o número de domínios temperature. \n",
    "\n",
    "\n",
    "Vamos tentar!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
